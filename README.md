# awesome-foundation-model-generalization



## Parameter/Annotation -Efficient Fine-Tuning

| Year | Venue | Title | Remark |
| :--- | :---: | :--- | :---: |
| 2024 | ICML    | [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/pdf/2401.01335) | [code](https://github.com/uclaml/SPIN), by UCLA| 
| 2025 | NeurIPS | [StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold](https://arxiv.org/abs/2510.01938v1) | [code](https://github.com/SonyResearch/stella) |


## Model Merging/Model Steering/Model Soup

| Year | Venue | Title | Remark |
| :--- | :---: | :--- | :---: |
| 2026 | arXiv| [Model soups need only one ingredient](https://arxiv.org/pdf/2602.09689) | [code](https://github.com/alirezaabdollahpour/MonoSoup) |
| 2025 | arXiv| [The Universal Weight Subspace Hypothesis](https://arxiv.org/pdf/2512.05117) | [code](https://toshi2k2.github.io/unisub/) |
| 2025 | arXiv| [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/pdf/2511.13254v1) | [code](https://github.com/facebookresearch/llm_souping), by ![Meta](https://static.xx.fbcdn.net/rsrc.php/y3/r/y6QsbGgc866.svg) |
| 2025 | arXiv| [Dynamically Scaled Activation Steering](https://arxiv.org/pdf/2512.03661) | by Apple |
| 2025 | CVPR | [Task Singular Vectors: Reducing Task Interference in Model Merging](https://arxiv.org/pdf/2412.00081) | [code](https://github.com/AntoAndGar/task_singular_vectors) |

## Large Reasoning Model
| Year | Venue | Title | Remark |
| :--- | :---: | :--- | :---: |
| 2025 | arXiv | [LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!](https://arxiv.org/pdf/2502.07374) | [code](https://github.com/NovaSky-AI/SkyThought), by UC Berkeley |
| 2025 | arXiv | [Reasoning with Sampling: Your Base Model is Smarter Than You Think](https://arxiv.org/pdf/2510.14901) | [code](https://github.com/aakaran/reasoning-with-sampling), by Harvard |
| 2025 | NeurIPS | [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://openreview.net/forum?id=4OsgYD7em5) | [code](https://github.com/LeapLabTHU/limit-of-RLVR), by THU |

## Diffusion Model
| Year | Venue | Title | Remark |
| :--- | :---: | :--- | :---: |
| 2026 | arXiv | [Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders](https://arxiv.org/pdf/2601.16208) | [code](https://github.com/ZitengWangNYU/Scale-RAE), by Saining Xie |
| 2025 | arXiv | [Diffusion Transformers with Representation Autoencoders (RAE)](https://arxiv.org/abs/2510.11690)) | [code](https://github.com/bytetriper/RAE), by Saining Xie |


